Bootstrap: docker
From: nvidia/cuda:12.1.0-devel-ubuntu22.04

%files
    pyproject.toml /transcriptomic-fms/
    README.md /transcriptomic-fms/
    transcriptomic_fms /transcriptomic-fms/transcriptomic_fms

%post
    # Suppress debconf and update-alternatives warnings
    export DEBIAN_FRONTEND=noninteractive
    export UCF_FORCE_CONFFNEW=1
    
    # Set CUDA and uv paths (set once at the beginning)
    export PATH="/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
    
    # Update package lists and install system dependencies
    # Retry apt-get update in case of transient mirror sync issues
    for i in 1 2 3; do
        if apt-get update -qq; then
            break
        else
            if [ $i -eq 3 ]; then
                echo "ERROR: apt-get update failed after 3 attempts"
                exit 1
            fi
            echo "apt-get update failed, retrying in 5 seconds... (attempt $i/3)"
            sleep 5
        fi
    done
    apt-get install -y -qq --no-install-recommends \
        build-essential \
        git \
        curl \
        ca-certificates \
        python3.11 \
        python3.11-dev \
        python3-pip \
        python3-venv \
        libhdf5-dev \
        libhdf5-103 \
        libpng-dev \
        libfreetype6-dev \
        libblas-dev \
        liblapack-dev \
        pkg-config
    
    # Create symlink for python
    ln -sf /usr/bin/python3.11 /usr/bin/python || true
    ln -sf /usr/bin/python3.11 /usr/bin/python3 || true
    
    # Clean up apt lists to reduce image size
    rm -rf /var/lib/apt/lists/*
    
    # Update library cache so system libraries can be found
    ldconfig
    
    # Install uv and add to PATH
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="/root/.local/bin:$PATH"

    # Set working directory
    cd /transcriptomic-fms

    # Test network connection to PyTorch download servers
    echo "Testing network connection to PyTorch download servers..."
    if curl -s -o /dev/null -w "Connected in %{time_connect}s, total: %{time_total}s\n" \
        --max-time 10 \
        https://download.pytorch.org/whl/cu121/ 2>/dev/null; then
        echo "Network connectivity OK"
    else
        echo "Warning: Connection test failed - network may be slow"
        echo "If PyTorch installation hangs, network speed may be the issue."
    fi

    # Install PyTorch with CUDA support first (before other dependencies)
    echo "Installing PyTorch with CUDA support (downloading wheel, should be fast)..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        "torch>=2.0.0,<2.3.0" || {
        echo "ERROR: PyTorch installation failed!"
        exit 1
    }
    echo "PyTorch installed successfully"

    # Install flash-attn using pre-built wheels (try wheel first, fall back to compilation)
    echo "Installing flash-attn..."
    python << 'PYEOF'
import subprocess
import sys
import torch

# Get PyTorch version and cxx11abi setting
torch_ver = torch.__version__.split('+')[0]
torch_major_minor = '.'.join(torch_ver.split('.')[:2])
cxx11abi = "TRUE" if torch._C._GLIBCXX_USE_CXX11_ABI else "FALSE"

# Map torch version to wheel version (cu122 wheels work with cu121)
torch_to_wheel = {
    "2.0": "2.1",  # Use 2.1 wheel for 2.0
    "2.1": "2.1",
    "2.2": "2.2",
    "2.3": "2.2",  # Use 2.2 wheel for 2.3
}

wheel_torch_ver = torch_to_wheel.get(torch_major_minor, "2.2")
wheel_url = (
    f"https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/"
    f"flash_attn-2.5.8%2Bcu122torch{wheel_torch_ver}cxx11abi{cxx11abi}-cp311-cp311-linux_x86_64.whl"
)

print(f"Trying pre-built wheel: torch {torch_major_minor}, cxx11abi={cxx11abi}")

# Try installing wheel
result = subprocess.run([
    "/root/.local/bin/uv", "pip", "install", "--system",
    "--preview-features", "extra-build-dependencies",
    wheel_url
], capture_output=True, text=True)

if result.returncode == 0:
    print("âœ“ Pre-built wheel installed successfully")
    sys.exit(0)
else:
    print(f"Wheel installation failed, falling back to compilation...")
    print(f"Error: {result.stderr[:200]}")
    sys.exit(1)
PYEOF

    # If Python script exited with 1, compile from source
    if [ $? -ne 0 ]; then
        echo "Compiling flash-attn from source (this will take 30-60 minutes)..."
        /root/.local/bin/uv pip install --system \
            --preview-features extra-build-dependencies \
            "flash-attn<1.0.5" || {
            echo "ERROR: flash-attn installation failed!"
            exit 1
        }
    fi
    
    echo "flash-attn installed successfully"

    # Install scGPT FIRST (following official installation from https://github.com/bowang-lab/scGPT)
    # This ensures all scGPT dependencies (including numpy<2) are installed correctly
    echo "Installing scGPT (official method: pip install scgpt 'flash-attn<1.0.5')..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        "scgpt" "flash-attn<1.0.5"
    
    # Install additional dependencies from your Colab setup
    echo "Installing additional scGPT dependencies..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        "gdown" "wandb" "python-louvain" "umap-learn<0.5.7" "pyarrow<14.0"
    
    # Install the package and dependencies from pyproject.toml (in editable mode)
    # uv should respect existing numpy<2 installation from scGPT
    echo "Installing package and dependencies from pyproject.toml..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        -e .
    
    # Ensure numpy<2 constraint is maintained (base package might try to upgrade)
    echo "Ensuring numpy<2 constraint for scGPT compatibility..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        "numpy<2" || true
    
    # Rebuild h5py from source to ensure it links against the installed HDF5 version
    # h5py wheels may be built against a different HDF5 version than what's in the container
    echo "Rebuilding h5py from source to match system HDF5 libraries..."
    
    # Ubuntu provides libhdf5_serial.so but h5py looks for libhdf5.so
    # Create symlinks so h5py can find the libraries
    cd /lib/x86_64-linux-gnu
    if [ -f "libhdf5_serial.so.103" ] && [ ! -f "libhdf5.so.103" ]; then
        ln -sf libhdf5_serial.so.103 libhdf5.so.103
        ln -sf libhdf5_serial.so libhdf5.so
    fi
    if [ -f "libhdf5_serial_hl.so.100" ] && [ ! -f "libhdf5_hl.so.100" ]; then
        ln -sf libhdf5_serial_hl.so.100 libhdf5_hl.so.100
        ln -sf libhdf5_serial_hl.so libhdf5_hl.so
    fi
    cd /transcriptomic-fms
    
    # Update library cache
    ldconfig
    
    # Set HDF5 environment for h5py build - use ONLY one method (h5py doesn't allow multiple)
    # Option 1: Use HDF5_DIR prefix (simplest)
    # h5py will look in /usr/include and /usr/lib for headers/libs
    # But serial HDF5 is in /usr/include/hdf5/serial, so we need to point there
    # Option 2: Use HDF5_INCLUDEDIR and HDF5_LIBDIR (without HDF5_DIR)
    # Option 3: Use pkg-config (if available)
    
    # Try using just HDF5_DIR first, pointing to where serial HDF5 is
    # But h5py expects standard layout, so let's use include/lib dirs instead
    unset HDF5_DIR || true
    export HDF5_INCLUDEDIR=/usr/include/hdf5/serial
    export HDF5_LIBDIR=/usr/lib/x86_64-linux-gnu
    # Don't set PKG_CONFIG_PATH if we're using include/lib dirs
    
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --force-reinstall --no-binary h5py h5py || {
        echo "ERROR: h5py rebuild failed! This is required for HDF5 compatibility."
        echo "Checking HDF5 installation..."
        ldconfig -p | grep hdf5 || echo "No HDF5 libraries found in cache"
        find /usr/lib -name "*libhdf5*" 2>/dev/null | head -5 || echo "No HDF5 libraries found"
        exit 1
    }
    
    # Verify h5py can import
    echo "Verifying h5py installation..."
    python -c "import h5py; print(f'h5py version: {h5py.__version__}')" || {
        echo "ERROR: h5py import failed after rebuild!"
        exit 1
    }
    
    # Verify numpy version matches scGPT requirements
    echo "Verifying numpy version..."
    python -c "import numpy; print(f'numpy version: {numpy.__version__}')" || echo "WARNING: Could not verify numpy version"

    # Verify torch is still the CUDA version
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        --upgrade-package torch "torch>=2.0.0,<2.3.0" || true

    # Create data and output directories
    mkdir -p /transcriptomic-fms/data
    mkdir -p /transcriptomic-fms/output
    mkdir -p /transcriptomic-fms/models

%environment
    export PATH="/root/.local/bin:/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib:$LD_LIBRARY_PATH"
    export PYTHONPATH="/transcriptomic-fms:$PYTHONPATH"
    export PYTHONUNBUFFERED=1
    export PYTHONNOUSERSITE=1
    export HDF5_DIR=/usr

%runscript
    # Default command
    exec python -m transcriptomic_fms.cli.main "$@"

%labels
    Author "Ahmed Sallam"
    Version "0.1.0"
    Description "Transcriptomic Foundation Models - scGPT container with CUDA support"
