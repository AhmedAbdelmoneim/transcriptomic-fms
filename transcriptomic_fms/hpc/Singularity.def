Bootstrap: docker
From: nvidia/cuda:12.1.0-devel-ubuntu22.04

%files
    pyproject.toml /transcriptomic-fms/
    README.md /transcriptomic-fms/
    transcriptomic_fms /transcriptomic-fms/transcriptomic_fms

%post
    # Suppress debconf and update-alternatives warnings
    export DEBIAN_FRONTEND=noninteractive
    export UCF_FORCE_CONFFNEW=1
    
    # Set CUDA and uv paths (set once at the beginning)
    export PATH="/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
    
    # Update package lists and install system dependencies
    apt-get update -qq
    apt-get install -y -qq --no-install-recommends \
        build-essential \
        git \
        curl \
        ca-certificates \
        python3.11 \
        python3.11-dev \
        python3-pip \
        python3-venv \
        libhdf5-dev \
        libhdf5-103 \
        libpng-dev \
        libfreetype6-dev \
        libblas-dev \
        liblapack-dev \
        pkg-config
    
    # Create symlink for python
    ln -sf /usr/bin/python3.11 /usr/bin/python || true
    ln -sf /usr/bin/python3.11 /usr/bin/python3 || true
    
    # Update library cache so system libraries can be found
    ldconfig
    
    # Clean up apt lists to reduce image size
    rm -rf /var/lib/apt/lists/*

    # Install uv and add to PATH
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="/root/.local/bin:$PATH"

    # Set working directory
    cd /transcriptomic-fms

    # Test network connection to PyTorch download servers
    echo "Testing network connection to PyTorch download servers..."
    if curl -s -o /dev/null -w "Connected in %{time_connect}s, total: %{time_total}s\n" \
        --max-time 10 \
        https://download.pytorch.org/whl/cu121/ 2>/dev/null; then
        echo "Network connectivity OK"
    else
        echo "Warning: Connection test failed - network may be slow"
        echo "If PyTorch installation hangs, network speed may be the issue."
    fi

    # Install PyTorch with CUDA support first (before other dependencies)
    echo "Installing PyTorch with CUDA support (downloading wheel, should be fast)..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        "torch>=2.0.0,<2.3.0" || {
        echo "ERROR: PyTorch installation failed!"
        exit 1
    }
    echo "PyTorch installed successfully"

    # Install flash-attn using pre-built wheels (try wheel first, fall back to compilation)
    echo "Installing flash-attn..."
    python << 'PYEOF'
import subprocess
import sys
import torch

# Get PyTorch version and cxx11abi setting
torch_ver = torch.__version__.split('+')[0]
torch_major_minor = '.'.join(torch_ver.split('.')[:2])
cxx11abi = "TRUE" if torch._C._GLIBCXX_USE_CXX11_ABI else "FALSE"

# Map torch version to wheel version (cu122 wheels work with cu121)
torch_to_wheel = {
    "2.0": "2.1",  # Use 2.1 wheel for 2.0
    "2.1": "2.1",
    "2.2": "2.2",
    "2.3": "2.2",  # Use 2.2 wheel for 2.3
}

wheel_torch_ver = torch_to_wheel.get(torch_major_minor, "2.2")
wheel_url = (
    f"https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/"
    f"flash_attn-2.5.8%2Bcu122torch{wheel_torch_ver}cxx11abi{cxx11abi}-cp311-cp311-linux_x86_64.whl"
)

print(f"Trying pre-built wheel: torch {torch_major_minor}, cxx11abi={cxx11abi}")

# Try installing wheel
result = subprocess.run([
    "/root/.local/bin/uv", "pip", "install", "--system",
    "--preview-features", "extra-build-dependencies",
    wheel_url
], capture_output=True, text=True)

if result.returncode == 0:
    print("âœ“ Pre-built wheel installed successfully")
    sys.exit(0)
else:
    print(f"Wheel installation failed, falling back to compilation...")
    print(f"Error: {result.stderr[:200]}")
    sys.exit(1)
PYEOF

    # If Python script exited with 1, compile from source
    if [ $? -ne 0 ]; then
        echo "Compiling flash-attn from source (this will take 30-60 minutes)..."
        /root/.local/bin/uv pip install --system \
            --preview-features extra-build-dependencies \
            "flash-attn<1.0.5" || {
            echo "ERROR: flash-attn installation failed!"
            exit 1
        }
    fi
    
    echo "flash-attn installed successfully"

    # Install the package and all dependencies from pyproject.toml (in editable mode)
    echo "Installing package and dependencies from pyproject.toml..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        -e .
    
    # Rebuild h5py from source to ensure it links against the installed HDF5 version
    # h5py wheels may be built against a different HDF5 version than what's in the container
    echo "Rebuilding h5py from source to match system HDF5 libraries..."
    # Set HDF5 environment variables for the build
    export HDF5_DIR=/usr
    export HDF5_VERSION=1.10.3
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --force-reinstall --no-binary h5py h5py || {
        echo "ERROR: h5py rebuild failed! This is required for HDF5 compatibility."
        echo "Checking HDF5 installation..."
        ldconfig -p | grep hdf5 || echo "No HDF5 libraries found in cache"
        find /usr/lib -name "*libhdf5*" 2>/dev/null | head -5 || echo "No HDF5 libraries found"
        exit 1
    }
    
    # Verify h5py can import
    echo "Verifying h5py installation..."
    python -c "import h5py; print(f'h5py version: {h5py.__version__}')" || {
        echo "ERROR: h5py import failed after rebuild!"
        exit 1
    }

    # Verify torch is still the CUDA version
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        --upgrade-package torch "torch>=2.0.0,<2.3.0" || true

    # Create data and output directories
    mkdir -p /transcriptomic-fms/data
    mkdir -p /transcriptomic-fms/output
    mkdir -p /transcriptomic-fms/models

%environment
    export PATH="/root/.local/bin:/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib:$LD_LIBRARY_PATH"
    export PYTHONPATH="/transcriptomic-fms:$PYTHONPATH"
    export PYTHONUNBUFFERED=1
    export HDF5_DIR=/usr

%runscript
    # Default command
    exec python -m transcriptomic_fms.cli.main "$@"

%labels
    Author "Ahmed Sallam"
    Version "0.1.0"
    Description "Transcriptomic Foundation Models - Embedding generation infrastructure"

