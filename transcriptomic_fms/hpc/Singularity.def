Bootstrap: docker
From: nvidia/cuda:12.1.0-devel-ubuntu22.04

%files
    pyproject.toml /transcriptomic-fms/
    README.md /transcriptomic-fms/
    transcriptomic_fms /transcriptomic-fms/transcriptomic_fms

%post
    # Suppress debconf and update-alternatives warnings
    export DEBIAN_FRONTEND=noninteractive
    export UCF_FORCE_CONFFNEW=1
    
    # Set CUDA and uv paths (set once at the beginning)
    export PATH="/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
    
    # Update package lists and install system dependencies
    apt-get update -qq
    apt-get install -y -qq --no-install-recommends \
        build-essential \
        git \
        curl \
        ca-certificates \
        python3.11 \
        python3.11-dev \
        python3-pip \
        python3-venv \
        libhdf5-dev \
        libhdf5-103 \
        libpng-dev \
        libfreetype6-dev \
        libblas-dev \
        liblapack-dev \
        pkg-config
    
    # Create symlink for python
    ln -sf /usr/bin/python3.11 /usr/bin/python || true
    ln -sf /usr/bin/python3.11 /usr/bin/python3 || true
    
    # Clean up apt lists to reduce image size
    rm -rf /var/lib/apt/lists/*

    # Install uv and add to PATH
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="/root/.local/bin:$PATH"

    # Set working directory
    cd /transcriptomic-fms

    # Test network connection and speed to PyTorch download servers
    echo "Testing network connection to PyTorch download servers..."
    echo "Testing connectivity (this should be fast)..."
    time curl -s -o /dev/null -w "Connected in %{time_connect}s, total: %{time_total}s\n" \
        --max-time 10 \
        https://download.pytorch.org/whl/cu121/ || \
        echo "Warning: Connection test failed - network may be slow"
    echo "If PyTorch installation hangs, network speed may be the issue."

    # Install PyTorch with CUDA support first (before other dependencies)
    echo "Installing PyTorch with CUDA support (downloading wheel, should be fast)..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        "torch>=2.0.0,<2.3.0" || {
        echo "ERROR: PyTorch installation failed!"
        exit 1
    }
    echo "PyTorch installed successfully"

    # Install flash-attn using pre-built wheels (much faster than compilation)
    echo "Installing flash-attn using pre-built wheels..."
    
    # Detect PyTorch version and cxx11abi setting to select the right wheel
    TORCH_VERSION=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "")
    CUDA_VERSION=$(python -c "import torch; print(torch.version.cuda)" 2>/dev/null || echo "12.1")
    CXX11ABI=$(python -c "import torch; print(torch._C._GLIBCXX_USE_CXX11_ABI)" 2>/dev/null || echo "False")
    
    echo "Detected: PyTorch $TORCH_VERSION, CUDA $CUDA_VERSION, cxx11abi=$CXX11ABI"
    
    # Determine which wheel to use
    # Flash-attn wheels use format: flash_attn-VERSION+cuXXXtorchYYYcxx11abiBOOL-cp311-cp311-linux_x86_64.whl
    # We'll try to find a compatible wheel for cu121/cu122 and torch 2.0-2.3
    FLASH_ATTN_WHEEL_URL=""
    
    # Try to determine torch major.minor version
    TORCH_MAJOR_MINOR=$(echo "$TORCH_VERSION" | sed -n 's/^\([0-9]\+\.[0-9]\+\).*/\1/p')
    CXX11ABI_STR="FALSE"
    if [ "$CXX11ABI" = "True" ] || [ "$CXX11ABI" = "1" ]; then
        CXX11ABI_STR="TRUE"
    fi
    
    # Use flash-attn 2.5.8 which has wheels for cu122 and torch 2.1/2.2/2.3 (compatible with cu121)
    # cu122 wheels should work with cu121 (minor version difference)
    if [ "$TORCH_MAJOR_MINOR" = "2.2" ] || [ "$TORCH_MAJOR_MINOR" = "2.3" ]; then
        # Use torch 2.2 wheel (should work with 2.2 and 2.3)
        FLASH_ATTN_WHEEL_URL="https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8%2Bcu122torch2.2cxx11abi${CXX11ABI_STR}-cp311-cp311-linux_x86_64.whl"
        echo "Using pre-built wheel: flash-attn 2.5.8 for torch $TORCH_MAJOR_MINOR, cu122, cxx11abi=$CXX11ABI_STR"
    elif [ "$TORCH_MAJOR_MINOR" = "2.1" ]; then
        # Use torch 2.1 wheel
        FLASH_ATTN_WHEEL_URL="https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8%2Bcu122torch2.1cxx11abi${CXX11ABI_STR}-cp311-cp311-linux_x86_64.whl"
        echo "Using pre-built wheel: flash-attn 2.5.8 for torch 2.1, cu122, cxx11abi=$CXX11ABI_STR"
    elif [ "$TORCH_MAJOR_MINOR" = "2.0" ]; then
        # Use torch 2.1 wheel (should work with 2.0)
        FLASH_ATTN_WHEEL_URL="https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8%2Bcu122torch2.1cxx11abi${CXX11ABI_STR}-cp311-cp311-linux_x86_64.whl"
        echo "Using pre-built wheel: flash-attn 2.5.8 for torch 2.0 (using 2.1 wheel), cu122, cxx11abi=$CXX11ABI_STR"
    else
        echo "Warning: Unsupported PyTorch version $TORCH_MAJOR_MINOR, falling back to compilation"
        FLASH_ATTN_WHEEL_URL=""
    fi
    
    # Try to install pre-built wheel if available
    WHEEL_INSTALLED=0
    if [ -n "$FLASH_ATTN_WHEEL_URL" ]; then
        echo "Downloading and installing pre-built wheel (this should be fast)..."
        if /root/.local/bin/uv pip install --system \
            --preview-features extra-build-dependencies \
            "$FLASH_ATTN_WHEEL_URL"; then
            WHEEL_INSTALLED=1
            echo "Pre-built wheel installed successfully"
        else
            echo "Warning: Pre-built wheel installation failed, falling back to compilation..."
        fi
    fi
    
    # Fall back to compilation if wheel installation failed or no compatible wheel found
    if [ "$WHEEL_INSTALLED" = "0" ]; then
        echo "Compiling flash-attn from source (this will take 30-60 minutes)..."
        NVCC_VERSION=$(nvcc --version 2>/dev/null | grep "release" | sed -n 's/.*release \([0-9.]*\).*/\1/p' | head -1 || echo "unknown")
        echo "nvcc found (CUDA $NVCC_VERSION)"
        /root/.local/bin/uv pip install --system \
            --preview-features extra-build-dependencies \
            "flash-attn<1.0.5" || {
            echo "ERROR: flash-attn installation failed!"
            exit 1
        }
    fi
    
    echo "flash-attn installed successfully"

    # Install the package and all dependencies from pyproject.toml (in editable mode)
    echo "Installing package and dependencies from pyproject.toml..."
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        -e .

    # Verify torch is still the CUDA version
    /root/.local/bin/uv pip install --system \
        --preview-features extra-build-dependencies \
        --index-url https://download.pytorch.org/whl/cu121 \
        --upgrade-package torch "torch>=2.0.0,<2.3.0" || true

    # Create data and output directories
    mkdir -p /transcriptomic-fms/data
    mkdir -p /transcriptomic-fms/output
    mkdir -p /transcriptomic-fms/models

%environment
    export PATH="/root/.local/bin:/usr/local/cuda/bin:$PATH"
    export LD_LIBRARY_PATH="/usr/local/cuda/lib64:$LD_LIBRARY_PATH"
    export PYTHONPATH="/transcriptomic-fms:$PYTHONPATH"
    export PYTHONUNBUFFERED=1

%runscript
    # Default command
    exec python -m transcriptomic_fms.cli.main "$@"

%labels
    Author "Ahmed Sallam"
    Version "0.1.0"
    Description "Transcriptomic Foundation Models - Embedding generation infrastructure"

